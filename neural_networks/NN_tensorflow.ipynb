{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aABxlw_GatoR",
        "outputId": "5b3aaa77-4974-4ea9-a281-bef166001e2c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# os.chdir(\"gdrive/MyDrive\")"
      ],
      "metadata": {
        "id": "MYXdoZqkdnOd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./'Colab Notebooks'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNWuQUI8bvzP",
        "outputId": "7f89f66e-55c3-46eb-93b7-9f725b218464"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: './Colab Notebooks'\n",
            "/content/gdrive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UCPSc_i8dMwW",
        "outputId": "7c60d971-4f25-4ead-82d9-51331716dc6a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup, importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report as cr, confusion_matrix as cm\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "on-lSX9ZdwQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3788b22-7646-485a-9d58-4f174242b352"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading/preprocessing data\n",
        "\n",
        "# file paths\n",
        "train_path = \"/content/gdrive/MyDrive/data/mnist_train.csv\"\n",
        "test_path = \"/content/gdrive/MyDrive/data/mnist_test.csv\"\n",
        "\n",
        "# loading data\n",
        "train_data = pd.read_csv(train_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Print to check the upload\n",
        "print(\"First 5 rows of training data:\")\n",
        "print(train_data.head())\n",
        "print(\"First 5 rows of test data:\")\n",
        "print(test_data.head())\n",
        "\n",
        "# separating features and labels\n",
        "X_train = train_data.iloc[:, 1:].values / 255.0 # Normalizing pixel values\n",
        "Y_train = train_data.iloc[:, 0].values\n",
        "X_test = test_data.iloc[:, 1:].values / 255.0 # Normalizing pixel values\n",
        "Y_test = test_data.iloc[:, 0].values\n",
        "\n",
        "# Print the shape of the datasets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of Y_train:\", Y_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of Y_test:\", Y_test.shape)\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, num_classes=10)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, num_classes=10)\n",
        "\n",
        "# Print the first 5 one-hot encoded labels to check\n",
        "print(\"First 5 one-hot encoded labels for Y_train:\")\n",
        "print(Y_train[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q49RZiHAi2H7",
        "outputId": "603b7c78-7622-417e-ff3e-4c750b7c0789"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of training data:\n",
            "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
            "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "\n",
            "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
            "0      0      0      0      0      0      0      0      0  \n",
            "1      0      0      0      0      0      0      0      0  \n",
            "2      0      0      0      0      0      0      0      0  \n",
            "3      0      0      0      0      0      0      0      0  \n",
            "4      0      0      0      0      0      0      0      0  \n",
            "\n",
            "[5 rows x 785 columns]\n",
            "First 5 rows of test data:\n",
            "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
            "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "\n",
            "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
            "0      0      0      0      0      0      0      0      0  \n",
            "1      0      0      0      0      0      0      0      0  \n",
            "2      0      0      0      0      0      0      0      0  \n",
            "3      0      0      0      0      0      0      0      0  \n",
            "4      0      0      0      0      0      0      0      0  \n",
            "\n",
            "[5 rows x 785 columns]\n",
            "Shape of X_train: (60000, 784)\n",
            "Shape of Y_train: (60000,)\n",
            "Shape of X_test: (10000, 784)\n",
            "Shape of Y_test: (10000,)\n",
            "First 5 one-hot encoded labels for Y_train:\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Neural Network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=( 784,)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# verifying model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hGU4plvlcvM",
        "outputId": "21f50753-cab1-4d31-9a75-955f5fb5c235"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109386 (427.29 KB)\n",
            "Trainable params: 109386 (427.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"Model compiled succesfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E3AY9Sgy9O7",
        "outputId": "2462ad36-c46e-4830-a72b-4747801a4997"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled succesfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# filepath: Where to save the model file.\n",
        "# monitor: Quantity to monitor (e.g., 'val_loss').\n",
        "# save_best_only: If True, saves only the best model (based on the monitored quantity).\n",
        "# save_weights_only: If True, saves only the model weights instead of the full model.\n",
        "# mode: One of {'auto', 'min', 'max'}; decides whether to minimize or maximize the monitored quantity.\n",
        "# verbose: Verbosity mode.\n",
        "checkpoint = ModelCheckpoint('model_checkpoint.h5',\n",
        "                             monitor='val_loss',\n",
        "                             save_best_only=True,\n",
        "                             save_weights_only=False,\n",
        "                             mode='min',\n",
        "                             verbose=1)\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, Y_train, epochs=10, batch_size=32,\n",
        "                    validation_data=(X_test, Y_test), callbacks=[checkpoint])\n",
        "\n",
        "# Print the keys of the training history to check the training process\n",
        "print(\"Training history keys:\", history.history.keys())\n",
        "\n",
        "# weights_specific_layer = model.get_layer('layer_name').get_weights()\n",
        "weights_all = model.get_weights()\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Save model weights\n",
        "# Checkpointing: You can save the current state of the model's weights during or after training. This is useful for resuming training or for future inference without needing to retrain the model.\n",
        "# Experimentation: When running multiple experiments, you can save weights at different stages to compare performance.\n",
        "# Deployment: Saved weights can be loaded into the model for deployment in a production environment.\n",
        "model.save_weights('model_weights.h5')\n",
        "\n",
        "# Load model weights\n",
        "# Resuming Training: If training was interrupted, you can resume from the last saved state.\n",
        "# Inference: Load pre-trained weights to use the model for making predictions without needing to retrain.\n",
        "# Testing and Validation: Load specific saved weights to validate or test the model performance under those specific conditions.\n",
        "# model.load_weights('model_weights.h5')\n",
        "\n",
        "# Google Colab saves training weights. Restart session to train afresh.\n",
        "\n",
        "# During an epoch, the model iterates over each batch of the dataset.\n",
        "# For each batch, the following steps occur -\n",
        "# Forward Pass: The model makes predictions on the batch of samples.\n",
        "# Loss Calculation: The loss function calculates the error between the predictions and the true labels.\n",
        "# Backward Pass: Backpropagation computes the gradients of the loss with respect to the model's weights.\n",
        "# Weight Update: The optimizer updates the model's weights based on the gradients.\n",
        "\n",
        "# History object, which is stored in the history variable. This object contains a record of training loss values\n",
        "# and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
        "\n",
        "# History object can be used to plot training and validation metrics\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Plot training & validation accuracy values\n",
        "# plt.plot(history.history['accuracy'])\n",
        "# plt.plot(history.history['val_accuracy'])\n",
        "# plt.title('Model accuracy')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot training & validation loss values\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('Model loss')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m-D8z3izQjC",
        "outputId": "2c213e24-2100-4a29-f379-376317c46c44"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.2447 - accuracy: 0.9279\n",
            "Epoch 1: val_loss improved from inf to 0.12710, saving model to model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 10s 5ms/step - loss: 0.2446 - accuracy: 0.9279 - val_loss: 0.1271 - val_accuracy: 0.9601\n",
            "Epoch 2/10\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9688\n",
            "Epoch 2: val_loss improved from 0.12710 to 0.09540, saving model to model_checkpoint.h5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1006 - accuracy: 0.9688 - val_loss: 0.0954 - val_accuracy: 0.9709\n",
            "Epoch 3/10\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9781\n",
            "Epoch 3: val_loss improved from 0.09540 to 0.08939, saving model to model_checkpoint.h5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0702 - accuracy: 0.9781 - val_loss: 0.0894 - val_accuracy: 0.9706\n",
            "Epoch 4/10\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.0546 - accuracy: 0.9827\n",
            "Epoch 4: val_loss improved from 0.08939 to 0.07069, saving model to model_checkpoint.h5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0545 - accuracy: 0.9827 - val_loss: 0.0707 - val_accuracy: 0.9790\n",
            "Epoch 5/10\n",
            "1863/1875 [============================>.] - ETA: 0s - loss: 0.0421 - accuracy: 0.9863\n",
            "Epoch 5: val_loss did not improve from 0.07069\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0421 - accuracy: 0.9863 - val_loss: 0.0761 - val_accuracy: 0.9756\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9890\n",
            "Epoch 6: val_loss did not improve from 0.07069\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0333 - accuracy: 0.9890 - val_loss: 0.0791 - val_accuracy: 0.9778\n",
            "Epoch 7/10\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.0283 - accuracy: 0.9904\n",
            "Epoch 7: val_loss did not improve from 0.07069\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0283 - accuracy: 0.9904 - val_loss: 0.0899 - val_accuracy: 0.9759\n",
            "Epoch 8/10\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.0236 - accuracy: 0.9922\n",
            "Epoch 8: val_loss did not improve from 0.07069\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 0.0812 - val_accuracy: 0.9792\n",
            "Epoch 9/10\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9925\n",
            "Epoch 9: val_loss did not improve from 0.07069\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0224 - accuracy: 0.9925 - val_loss: 0.0885 - val_accuracy: 0.9780\n",
            "Epoch 10/10\n",
            "1860/1875 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9939\n",
            "Epoch 10: val_loss did not improve from 0.07069\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.0897 - val_accuracy: 0.9767\n",
            "Training history keys: dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109386 (427.29 KB)\n",
            "Trainable params: 109386 (427.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the model\n",
        "train_loss, train_accuracy = model.evaluate(X_train, Y_train, verbose=0)\n",
        "test_loss, test_accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(f\"Training Accuracy : {train_accuracy}\")\n",
        "print(f\"Test Accuracy : {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAOEpWpwP6Sd",
        "outputId": "f6872657-f5d9-4eb1-f533-250fe1ee4e9f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy : 0.9949333071708679\n",
            "Test Accuracy : 0.9767000079154968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating predictions\n",
        "train_predictions = model.predict(X_train)\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# np.argmax is a NumPy function that returns the indices of the maximum values along a\n",
        "# specified axis. Models often output probabilities for each class in classification problems.\n",
        "# To convert these probabilities into actual class predictions, np.argmax is used to find the\n",
        "# class with the highest probability\n",
        "train_pred_classes = np.argmax(train_predictions, axis=1)\n",
        "test_pred_classes = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "Y_train_classes = np.argmax(Y_train, axis=1)\n",
        "Y_test_classes = np.argmax(Y_test, axis=1)\n",
        "\n",
        "# Print the first 5 predictions and their corresponding true labels for training and test data\n",
        "print(\"First 5 training predictions:\", train_pred_classes[:5])\n",
        "print(\"First 5 training true labels:\", Y_train_classes[:5])\n",
        "print(\"First 5 test predictions:\", test_pred_classes[:5])\n",
        "print(\"First 5 test true labels:\", Y_test_classes[:5])"
      ],
      "metadata": {
        "id": "4K6yChjtQoWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "# Cell 8: Classification Report and Confusion Matrix for Training Data\n",
        "print(\"Training Classification Report:\")\n",
        "print(classification_report(Y_train_classes, train_pred_classes))\n",
        "\n",
        "print(\"Training Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_train_classes, train_pred_classes))"
      ],
      "metadata": {
        "id": "gVNcxtLKW3zp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}